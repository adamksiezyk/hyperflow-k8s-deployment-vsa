nodeSelector: {}


config:
  data:
    deployment.yml: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: {poolName}
        namespace: {namespace}
        labels:
          app: {poolName}
      spec:
        replicas: 0
        selector:
          matchLabels:
            app: {poolName}
        template:
          metadata:
            labels:
              app: {poolName}
          spec:
            terminationGracePeriodSeconds: 300
            containers:
              - name: worker
                image: {image}
                command: ["hflow-job-listener.js"]
                env:
                  - name: QUEUE_NAME
                    value: {queueName}
                  - name: RABBIT_HOSTNAME
                    value: rabbitmq.default
                  - name: REDIS_URL
                    value: redis://redis:6379
                  - name: RABBIT_PREFETCH_SIZE
                    value: '1'
                  - name: HF_VAR_WORK_DIR
                    value: /work_dir
                  - name: HF_VAR_WAIT_FOR_INPUT_FILES
                    value: "0"
                  - name: HF_VAR_NUM_RETRIES
                    value: "1"
                  - name: HF_LOG_NODE_NAME
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: spec.nodeName
                  - name: HF_LOG_POD_NAME
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: metadata.name
                  - name: HF_LOG_POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: metadata.namespace
                  - name: HF_LOG_POD_IP
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: status.podIP
                  - name: HF_LOG_POD_SERVICE_ACCOUNT
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: spec.serviceAccountName
                  - name: HF_VAR_FS_MONIT_ENABLED
                    value: "0"
                  - name: HF_VAR_FS_MONIT_COMMAND
                    value: hflow-job-execute $REDIS_URL -a -- 'Gjt5YxVCa:1:5:1' 'Gjt5YxVCa:1:6:1'
                  - name: HF_VAR_FS_MONIT_PATH_PATTERN
                    value: /work_dir/*
                imagePullPolicy: Always
                resources:
                  requests:
                    cpu: {cpuRequests}
                    memory: {memoryRequests}
      #            limits:
      #              cpu: {cpuLimits}
      #              memory: {memoryLimits}
                volumeMounts:
                  - mountPath: /work_dir
                    name: my-pvc-nfs
                  - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
                    name: kube-api-access-h4jjk
                    readOnly: true
                workingDir: /work_dir
            volumes:
              - name: my-pvc-nfs
                persistentVolumeClaim:
                  claimName: nfs
              - name: kube-api-access-h4jjk
                projected:
                  defaultMode: 420
                  sources:
                    - serviceAccountToken:
                        expirationSeconds: 3607
                        path: token
                    - configMap:
                        items:
                          - key: ca.crt
                            path: ca.crt
                        name: kube-root-ca.crt
                    - downwardAPI:
                        items:
                          - fieldRef:
                              apiVersion: v1
                              fieldPath: metadata.namespace
                            path: namespace
    prometheus-rule.yml: "apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n
      \ name: {poolName}\n  namespace: {namespace}\n  labels:\n    prometheus: kube-prometheus
      # This label is configured as port of the installation of the prometheus operator\n
      \   release: monitoring\nspec:\n  groups:\n    - name: \"{poolName}-replication-factor\"
      # Define the name of your rule\n      rules:\n        - record: {poolNameUnderscored}:replication_factor
      # The name of the metrics you want\n          expr: |\n            ceil(  \n              sum(\n
      \               (\n                  rabbitmq_queue_messages_ready{{job=\"amqp-metrics\",
      queue=\"{queueName}\"}}\n                  + \n                  rabbitmq_queue_messages_unacknowledged{{job=\"amqp-metrics\",
      queue=\"{queueName}\"}}\n                ) or vector(0)\n              )\n              /
      sum(last_over_time(rabbitmq_queue_messages_total{{job=\"amqp-metrics\"}}[15s])
      or vector(1))\n              *\n              min(\n                floor(sum(node:node_num_cpu:sum{{node=~\".+-default-pool-.*\"}}
      - 1) / {cpuLimits}) \n                or\n                floor(sum(node_memory_MemTotal_bytes
      * 0.9) / {memoryLimits})\n              )\n            )\n          labels:\n
      \           namespace: default\n            service: {poolName} # Name of the
      service you are going to configure your HPA with"
    scaledobject.yml: |-
      apiVersion: keda.sh/v1alpha1
      kind: ScaledObject
      metadata:
        name: {poolName}
        namespace: {namespace}
        labels:
          name: {poolName}
      spec:
        scaleTargetRef:
          name: {poolName}
        pollingInterval: 3
        cooldownPeriod:  0
        minReplicaCount: 0
        maxReplicaCount: 10
        triggers:
          - type: prometheus
            metadata:
              serverAddress: http://prometheus-operated.default:9090
              metricName: {poolNameUnderscored}:replication_factor
              threshold: '1'
              query: {poolNameUnderscored}:replication_factor{{}}
        advanced:
          horizontalPodAutoscalerConfig:
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 0
                policies:
                  - periodSeconds: 15
                    type: Pods
                    value: 10
              scaleDown:
                stabilizationWindowSeconds: 0
                policies:
                  - periodSeconds: 15
                    type: Pods
                    value: 10