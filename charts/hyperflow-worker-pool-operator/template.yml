---
# Source: hflow-worker-operator/templates/rbac.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: workers
  name: hflow-worker-operator-account
---
# Source: hflow-worker-operator/templates/configmap.yml
apiVersion: v1
data:
  deployment.yml: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: {poolName}
      namespace: {namespace}
      labels:
        app: {poolName}
    spec:
      replicas: 0
      selector:
        matchLabels:
          app: {poolName}
      template:
        metadata:
          labels:
            app: {poolName}
        spec:
          terminationGracePeriodSeconds: 300
          containers:
            - name: worker
              image: {image}
              command: ["hflow-job-listener.js"]
              env:
                - name: QUEUE_NAME
                  value: {queueName}
                - name: RABBIT_HOSTNAME
                  value: rabbitmq.default
                - name: REDIS_URL
                  value: redis://redis:6379
                - name: RABBIT_PREFETCH_SIZE
                  value: '1'
                - name: HF_VAR_WORK_DIR
                  value: /work_dir
                - name: HF_VAR_WAIT_FOR_INPUT_FILES
                  value: "0"
                - name: HF_VAR_NUM_RETRIES
                  value: "1"
                - name: HF_LOG_NODE_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.nodeName
                - name: HF_LOG_POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: HF_LOG_POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                - name: HF_LOG_POD_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.podIP
                - name: HF_LOG_POD_SERVICE_ACCOUNT
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.serviceAccountName
                - name: HF_VAR_FS_MONIT_ENABLED
                  value: "0"
                - name: HF_VAR_FS_MONIT_COMMAND
                  value: hflow-job-execute $REDIS_URL -a -- 'Gjt5YxVCa:1:5:1' 'Gjt5YxVCa:1:6:1'
                - name: HF_VAR_FS_MONIT_PATH_PATTERN
                  value: /work_dir/*
              imagePullPolicy: Always
              resources:
                requests:
                  cpu: {cpuRequests}
                  memory: {memoryRequests}
    #            limits:
    #              cpu: {cpuLimits}
    #              memory: {memoryLimits}
              volumeMounts:
                - mountPath: /work_dir
                  name: my-pvc-nfs
                - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
                  name: kube-api-access-h4jjk
                  readOnly: true
              workingDir: /work_dir
          volumes:
            - name: my-pvc-nfs
              persistentVolumeClaim:
                claimName: nfs
            - name: kube-api-access-h4jjk
              projected:
                defaultMode: 420
                sources:
                  - serviceAccountToken:
                      expirationSeconds: 3607
                      path: token
                  - configMap:
                      items:
                        - key: ca.crt
                          path: ca.crt
                      name: kube-root-ca.crt
                  - downwardAPI:
                      items:
                        - fieldRef:
                            apiVersion: v1
                            fieldPath: metadata.namespace
                          path: namespace
  prometheus-rule.yml: "apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n
    \ name: {poolName}\n  namespace: {namespace}\n  labels:\n    prometheus: kube-prometheus
    # This label is configured as port of the installation of the prometheus operator\n
    \   release: monitoring\nspec:\n  groups:\n    - name: \"{poolName}-replication-factor\"
    # Define the name of your rule\n      rules:\n        - record: {poolNameUnderscored}:replication_factor
    # The name of the metrics you want\n          expr: |\n            ceil(  \n              sum(\n
    \               (\n                  rabbitmq_queue_messages_ready{{job=\"amqp-metrics\",
    queue=\"{queueName}\"}}\n                  + \n                  rabbitmq_queue_messages_unacknowledged{{job=\"amqp-metrics\",
    queue=\"{queueName}\"}}\n                ) or vector(0)\n              )\n              /
    sum(last_over_time(rabbitmq_queue_messages_total{{job=\"amqp-metrics\"}}[15s]) or
    vector(1))\n              *\n              min(\n                floor(sum(node:node_num_cpu:sum{{node=~\".+-default-pool-.*\"}}
    - 1) / {cpuLimits}) \n                or\n                floor(sum(node_memory_MemTotal_bytes
    * 0.9) / {memoryLimits})\n              )\n            )\n          labels:\n            namespace:
    default\n            service: {poolName} # Name of the service you are going to
    configure your HPA with"
  scaledobject.yml: |-
    apiVersion: keda.sh/v1alpha1
    kind: ScaledObject
    metadata:
      name: {poolName}
      namespace: {namespace}
      labels:
        name: {poolName}
    spec:
      scaleTargetRef:
        name: {poolName}
      pollingInterval: 3
      cooldownPeriod:  0
      minReplicaCount: 0
      maxReplicaCount: 10
      triggers:
        - type: prometheus
          metadata:
            serverAddress: http://prometheus-operated.default:9090
            metricName: {poolNameUnderscored}:replication_factor
            threshold: '1'
            query: {poolNameUnderscored}:replication_factor{{}}
      advanced:
        horizontalPodAutoscalerConfig:
          behavior:
            scaleUp:
              stabilizationWindowSeconds: 0
              policies:
                - periodSeconds: 15
                  type: Pods
                  value: 10
            scaleDown:
              stabilizationWindowSeconds: 0
              policies:
                - periodSeconds: 15
                  type: Pods
                  value: 10
kind: ConfigMap
metadata:
  name: workerpool-deployment-templates
  namespace: workers
---
# Source: hflow-worker-operator/templates/rbac.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: hflow-worker-operator-role-cluster
rules:

  # Framework: knowing which other operators are running (i.e. peering).
  - apiGroups: [kopf.dev]
    resources: [clusterkopfpeerings]
    verbs: [list, watch, patch, get]

  # Framework: runtime observation of namespaces & CRDs (addition/deletion).
  - apiGroups: [apiextensions.k8s.io]
    resources: [customresourcedefinitions]
    verbs: [list, watch]
  - apiGroups: [""]
    resources: [namespaces]
    verbs: [list, watch]

  # Framework: admission webhook configuration management.
  - apiGroups: [admissionregistration.k8s.io/v1, admissionregistration.k8s.io/v1]
    resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
    verbs: [create, patch]

  # Application: read-only access for watching cluster-wide.
  - apiGroups: [kopf.dev]
    resources: [hflow-worker-operators]
    verbs: [list, watch]

  # Application: read-only access for watching cluster-wide.
  - apiGroups: [hyperflow.agh.edu.pl]
    resources: [workerpools]
    verbs: [get, list, watch, patch]
---
# Source: hflow-worker-operator/templates/rbac.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: hflow-worker-operator-rolebinding-cluster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hflow-worker-operator-role-cluster
subjects:
  - kind: ServiceAccount
    name: hflow-worker-operator-account
    namespace: workers
---
# Source: hflow-worker-operator/templates/rbac.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: workers
  name: hflow-worker-operator-role-namespaced
rules:

  # Framework: knowing which other operators are running (i.e. peering).
  - apiGroups: [kopf.dev]
    resources: [kopfpeerings]
    verbs: [list, watch, patch, get]

  # Framework: posting the events about the handlers progress/errors.
  - apiGroups: [""]
    resources: [events]
    verbs: [create]

  # Application: watching & handling for the custom resource we declare.
  - apiGroups: [kopf.dev]
    resources: [hflow-worker-operators]
    verbs: [list, watch, patch]

  # Application: other resources it produces and manipulates.
  # Here, we create Jobs+PVCs+Pods, but we do not patch/update/delete them ever.
  - apiGroups: [batch, extensions]
    resources: [jobs]
    verbs: [create]
  - apiGroups: [""]
    resources: [pods, persistentvolumeclaims]
    verbs: [create]
  - apiGroups: ["apps"]
    resources: [deployments]
    verbs: [get, list, watch, patch, create, delete]
---
# Source: hflow-worker-operator/templates/rbac.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: workers
  name: hflow-worker-operator-rolebinding-namespaced
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hflow-worker-operator-role-namespaced
subjects:
  - kind: ServiceAccount
    name: hflow-worker-operator-account
---
# Source: hflow-worker-operator/templates/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hflow-worker-operator
  namespace: workers
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      application: hflow-worker-operator
  template:
    metadata:
      labels:
        application: hflow-worker-operator
    spec:
      serviceAccountName: hflow-worker-operator-account
      containers:
        - name: operator
          image: kjanecki/hflow-worker-operator:latest
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /templates
              name: templates
      volumes:
        - name: templates
          configMap:
            name: workerpool-deployment-templates
